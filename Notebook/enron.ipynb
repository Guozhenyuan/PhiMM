{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENRON Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128103\n",
      "../Data/raw/maildir/scott-s/all_documents/628.\n",
      "Dataset({\n",
      "    features: ['email_path'],\n",
      "    num_rows: 128103\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "datadir = '../Data/raw/maildir'\n",
    "email_paths = glob.glob(f\"{datadir}/**/all_documents/*\")\n",
    "\n",
    "print(len(email_paths))\n",
    "print(email_paths[0])\n",
    "\n",
    "dataset = Dataset.from_dict({'email_path':email_paths})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6c6d56646e47cfbd943aa2af305144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/128103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['email_path', 'Message-ID', 'Date', 'From', 'To', 'Subject', 'Body', 'len'],\n",
      "    num_rows: 128103\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e18da2e902401cb629af1bc6ee3ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=48):   0%|          | 0/128103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['email_path', 'Message-ID', 'Date', 'From', 'To', 'Subject', 'Body', 'len'],\n",
      "    num_rows: 91672\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1363d4e0b2c946c1834be9a72ca369f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/91672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['email_path', 'Message-ID', 'Date', 'From', 'To', 'Subject', 'Body', 'len', 'unmask_seq', 'masked_seq', 'pii_mask_idx', 'pii_mask'],\n",
      "    num_rows: 91672\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157ee618f10347a99dfc6d71e48cf6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=48):   0%|          | 0/91672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['email_path', 'Message-ID', 'Date', 'From', 'To', 'Subject', 'Body', 'len', 'unmask_seq', 'masked_seq', 'pii_mask_idx', 'pii_mask'],\n",
      "    num_rows: 82137\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a767f6f9dd9444aae5e66ab8af0c6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=48):   0%|          | 0/82137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['email_path', 'Message-ID', 'Date', 'From', 'To', 'Subject', 'Body', 'len', 'unmask_seq', 'masked_seq', 'pii_mask_idx', 'pii_mask'],\n",
      "    num_rows: 45098\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['https_proxy'] = 'http://10.14.30.39:7890'\n",
    "import email\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine\n",
    "from presidio_analyzer.nlp_engine import TransformersNlpEngine\n",
    "from presidio_anonymizer.entities import RecognizerResult, OperatorResult, OperatorConfig\n",
    "from presidio_anonymizer.operators import Decrypt\n",
    "from presidio_anonymizer.entities import (\n",
    "    ConflictResolutionStrategy,\n",
    "    EngineResult,\n",
    "    OperatorConfig,\n",
    "    RecognizerResult,\n",
    ")\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from Data.utils import save_json\n",
    "\n",
    "import copy\n",
    "\n",
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n",
    "                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n",
    "}\n",
    "\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine_with_spanish = provider.create_engine()\n",
    "\n",
    "analyzer = AnalyzerEngine(default_score_threshold=0.85,nlp_engine=nlp_engine_with_spanish, \n",
    "    supported_languages=[\"en\", \"es\"])\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "\n",
    "\n",
    "def presidio_pii(seq,analyzer,anonymizer):\n",
    "    # print(example)\n",
    "    # ,entities=['PERSON', 'PHONE_NUMBER', 'DATE_TIME', 'LOCATION', 'EMAIL_ADDRESS', 'NRP']\n",
    "    analyze_result = analyzer.analyze(text=seq,language='en',entities=['PERSON', 'PHONE_NUMBER', 'DATE_TIME', 'LOCATION', 'EMAIL_ADDRESS', 'NRP'])\n",
    "\n",
    "    op_dict = {}\n",
    "    pii_mask_idx = []\n",
    "    pii_mask = []\n",
    "    pii_dict = {} # pii_type:[pii1,pii2]\n",
    "    masked_seq = copy.deepcopy(seq)\n",
    "    for az in analyze_result:\n",
    "        pii = seq[az.start:az.end]\n",
    "        pii_type = az.entity_type\n",
    "        if pii_type not in pii_dict.keys():\n",
    "            pii_dict[pii_type] = [pii]\n",
    "        else:\n",
    "            if pii not in pii_dict[pii_type]:\n",
    "                pii_dict[pii_type].append(pii)\n",
    "        idx = pii_dict[pii_type].index(pii)\n",
    "        pii_type_new = pii_type+'-'+str(idx)\n",
    "        masked_seq = masked_seq.replace(pii,f'[{pii_type_new}]')\n",
    "        \n",
    "        # print(az)\n",
    "        pii_mask_idx.append({'value':pii,'label':pii_type_new,'start':az.start,'end':az.end})\n",
    "        pii_mask.append((pii,pii_type_new))\n",
    "        # operators={az.entity_type: OperatorConfig(\"replace\", {az.entity_type:az.entity_type})}\n",
    "        # op_dict[pii_type_new] = OperatorConfig(\"replace\", {\"new_value\": f'[{pii_type_new}]'})\n",
    "    # print(analyze_result)\n",
    "    # anonymizer_result =  anonymizer.anonymize(text=seq,analyzer_results=analyze_result,operators=op_dict)\n",
    "    pii_mask = set(pii_mask)\n",
    "    pii_mask = [{'value':pm[0],'label':pm[1]} for pm in pii_mask]\n",
    "    # return anonymizer_result.text,pii_mask\n",
    "    return masked_seq,pii_mask,pii_mask_idx\n",
    "    # print(anonymizer_result)\n",
    "\n",
    "\n",
    "# 读取一个邮件文件\n",
    "def read_email(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        msg = BytesParser(policy=policy.default).parse(f)\n",
    "    return msg\n",
    "\n",
    "# 解析邮件内容\n",
    "def parse_email(msg):\n",
    "    email_data = {\n",
    "        \"Message-ID\": msg.get(\"Message-ID\"),\n",
    "        \"Date\": msg.get(\"Date\"),\n",
    "        \"From\": msg.get(\"From\"),\n",
    "        \"To\": msg.get(\"To\"),\n",
    "        \"Subject\": msg.get(\"Subject\"),\n",
    "        \"Body\": msg.get_body(preferencelist=('plain')).get_content() if msg.get_body(preferencelist=('plain')) else None\n",
    "    }\n",
    "    return email_data\n",
    "\n",
    "def proc_email(example):\n",
    "    path = example['email_path']\n",
    "    msg = read_email(path)\n",
    "    data = parse_email(msg)\n",
    "    body = data['Body']\n",
    "    data['len'] = len(body)\n",
    "    return data\n",
    "\n",
    "def proc_pii(example):\n",
    "    body = example['Body']\n",
    "    data = {}\n",
    "    data['unmask_seq'] = body\n",
    "    masked_seq,pii_mask,pii_mask_idx = presidio_pii(body,analyzer,anonymizer)\n",
    "    # print(masked_seq)\n",
    "    data['masked_seq'] = masked_seq\n",
    "    data['pii_mask_idx'] = pii_mask_idx\n",
    "    data['pii_mask'] = pii_mask\n",
    "    return data\n",
    "\n",
    "def filter_conflict(example):\n",
    "    pii_mask = example['pii_mask']\n",
    "    flag = 0\n",
    "    pii_dict = {}\n",
    "    for pm in pii_mask:\n",
    "        pii_type = pm['label']\n",
    "        pii = pm['value']\n",
    "        if pii_type not in pii_dict:\n",
    "            pii_dict[pii_type] = pii\n",
    "        else:\n",
    "            if pii_dict[pii_type] != pii:\n",
    "                flag=1\n",
    "    return flag==0\n",
    "\n",
    "def proc_pii_privacy(example):\n",
    "    pii_mask = example['pii_mask']\n",
    "    new_pii_mask = []\n",
    "    pii_dict = {} # pii_type:[pii1,pii2]\n",
    "    for pm in pii_mask:\n",
    "        pii_type = pm['label']\n",
    "        pii = pm['value']\n",
    "        if pii_type not in pii_dict.keys():\n",
    "            pii_dict[pii_type] = [pii]\n",
    "        else:\n",
    "            if pii not in pii_dict[pii_type]:\n",
    "                pii_dict[pii_type].append(pii)\n",
    "\n",
    "        idx = pii_dict[pii_type].index(pii)\n",
    "        pm['label'] = pm['label']+'-'+str(idx)\n",
    "    \n",
    "    return example\n",
    "\n",
    "def fliter_pii_too_much(example):\n",
    "    # 句子中90%单词不能是pii\n",
    "    len_unmask_seq = len(example['masked_seq'].split(' '))\n",
    "    len_pii_mask = len(example['pii_mask_idx'])\n",
    "    # 一句话中一种PII不应该超过三种以上\n",
    "    return len_pii_mask <= 0.1*len_unmask_seq\n",
    "\n",
    "\n",
    "# 处理获得email数据\n",
    "dataset_proc = dataset.map(proc_email,num_proc=48)\n",
    "print(dataset_proc)\n",
    "# 过滤email字符串长度超过1500的\n",
    "dataset_proc = dataset_proc.filter(lambda x: x['len']<1500,num_proc=48)\n",
    "print(dataset_proc)\n",
    "# 处理经过presidio_pii处理得到的masked_seq unmask_seq pii_mask\n",
    "dataset_proc = dataset_proc.map(proc_pii,num_proc=48)\n",
    "print(dataset_proc)\n",
    "# 过滤没有pii的邮件\n",
    "dataset_proc = dataset_proc.filter(lambda x: len(x['pii_mask'])>0, num_proc=48)\n",
    "print(dataset_proc)\n",
    "# 过滤pii有歧义的邮件\n",
    "# dataset_proc = dataset_proc.filter(filter_conflict, num_proc=48)\n",
    "# print(dataset_proc)\n",
    "# 为句子中每个不同的pii都分配一个类别\n",
    "# dataset_proc = dataset_proc.map(proc_pii_privacy,num_proc=48)\n",
    "# print(dataset_proc)\n",
    "# 过滤句子中90%单词不能是pii\n",
    "dataset_proc = dataset_proc.filter(fliter_pii_too_much,num_proc=48)\n",
    "print(dataset_proc)\n",
    "\n",
    "# 保存原始数据\n",
    "# path = '../Data/raw/enron.json'\n",
    "# dataset_proc.to_json(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f324ea0dbf49599eaf3eaa521f048c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=48):   0%|          | 0/45098 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['email_path', 'Message-ID', 'Date', 'From', 'To', 'Subject', 'Body', 'len', 'unmask_seq', 'masked_seq', 'pii_mask_idx', 'pii_mask'],\n",
      "    num_rows: 31946\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def filter_pii_type_too_much(example):\n",
    "    flag = 0\n",
    "    pii_mask=example['pii_mask']\n",
    "    # print(set(pii_mask))\n",
    "    for pm in pii_mask:\n",
    "        label = pm['label']\n",
    "        if '4' in label:\n",
    "            flag=1\n",
    "    return flag==0 and len(pii_mask)<7\n",
    "\n",
    "dataset_proc1 = dataset_proc.filter(filter_pii_type_too_much,num_proc=48)\n",
    "print(dataset_proc1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_type_dict ={}\n",
    "pii_num={}\n",
    "for dp in dataset_proc1:\n",
    "    pii_mask = dp['pii_mask']\n",
    "    len_pii_mask = len(pii_mask)\n",
    "    if len_pii_mask not in pii_num.keys():\n",
    "        pii_num[len_pii_mask]=1\n",
    "    else:\n",
    "        pii_num[len_pii_mask]+=1\n",
    "\n",
    "\n",
    "    for pm in pii_mask:\n",
    "        label = pm['label'].split('-')[0]\n",
    "        if label not in pii_type_dict.keys():\n",
    "            pii_type_dict[label]=1\n",
    "        else:\n",
    "            pii_type_dict[label]+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 8441, 2: 6908, 3: 5600, 5: 3557, 4: 4903, 6: 2537}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pii_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PERSON': 42979,\n",
       " 'DATE_TIME': 31564,\n",
       " 'LOCATION': 10696,\n",
       " 'EMAIL_ADDRESS': 3823,\n",
       " 'NRP': 2614}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pii_type_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分A和B并保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from Data.utils import save_json\n",
    "dataset_AB = dataset_proc1.train_test_split(test_size=0.5,seed=42)\n",
    "\n",
    "path = '../Data/raw/phishing/enron.json'\n",
    "\n",
    "save_json({'A':dataset_AB['train'].to_list(),'B':dataset_AB['test'].to_list()},path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PERSON': 42979,\n",
       " 'DATE_TIME': 31564,\n",
       " 'LOCATION': 10696,\n",
       " 'EMAIL_ADDRESS': 3823,\n",
       " 'NRP': 2614}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pii_dict = {}\n",
    "for pm in dataset_proc1['pii_mask']:\n",
    "    for p in pm:\n",
    "        label = p['label'].split(\"-\")[0]\n",
    "        value = p['value']\n",
    "        if label not in pii_dict.keys():\n",
    "            pii_dict[label] = 1\n",
    "        else:\n",
    "            pii_dict[label] +=1\n",
    "pii_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persidio Try\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4715309a304b1dbaaaaba62f00ece4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zju_wck/miniconda3/envs/llm-merging/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=45, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['https_proxy'] = 'http://10.14.30.39:7890'\n",
    "\n",
    "import transformers\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "transformers_model = \"obi/deid_roberta_i2b2\" # e.g. \"obi/deid_roberta_i2b2\"\n",
    "\n",
    "snapshot_download(repo_id=transformers_model)\n",
    "\n",
    "# Instantiate to make sure it's downloaded during installation and not runtime\n",
    "AutoTokenizer.from_pretrained(transformers_model)\n",
    "AutoModelForTokenClassification.from_pretrained(transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;1m✘ No compatible package found for 'urchade/gliner_multi_pii-v1' (spaCy\n",
      "v3.7.6)\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zju_wck/miniconda3/envs/llm-merging/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "# Create configuration containing engine name and models\n",
    "conf_file = './parser.yaml'\n",
    "\n",
    "# Create NLP engine based on configuration\n",
    "provider = NlpEngineProvider(conf_file=conf_file)\n",
    "nlp_engine = provider.create_engine()\n",
    "\n",
    "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine=nlp_engine, \n",
    "    supported_languages=[\"en\"]\n",
    ")\n",
    "\n",
    "results_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\n",
    "print(results_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLiNER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94315c6fdc2a4cb1ad66410fa96ef213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 1, 'end': 18, 'text': 'Cristiano Ronaldo', 'label': 'person', 'score': 0.8923935294151306}, {'start': 92, 'end': 107, 'text': '5 February 1985', 'label': 'date', 'score': 0.9758335947990417}, {'start': 233, 'end': 255, 'text': 'Portugal national team', 'label': 'teams', 'score': 0.7006867527961731}, {'start': 317, 'end': 324, 'text': 'Ronaldo', 'label': 'person', 'score': 0.5729479193687439}, {'start': 338, 'end': 356, 'text': \"Ballon d'Or awards\", 'label': 'award', 'score': 0.6336824893951416}, {'start': 381, 'end': 417, 'text': \"UEFA Men's Player of the Year Awards\", 'label': 'award', 'score': 0.8929407000541687}, {'start': 428, 'end': 449, 'text': 'European Golden Shoes', 'label': 'award', 'score': 0.8832416534423828}, {'start': 556, 'end': 578, 'text': 'UEFA Champions Leagues', 'label': 'competitions', 'score': 0.7971668243408203}, {'start': 584, 'end': 610, 'text': 'UEFA European Championship', 'label': 'competitions', 'score': 0.9217649698257446}, {'start': 619, 'end': 638, 'text': 'UEFA Nations League', 'label': 'competitions', 'score': 0.9559527635574341}, {'start': 640, 'end': 647, 'text': 'Ronaldo', 'label': 'person', 'score': 0.5068790912628174}, {'start': 761, 'end': 782, 'text': 'European Championship', 'label': 'competitions', 'score': 0.5458036065101624}]\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_large-v2.1\")\n",
    "\n",
    "text = \"\"\"\n",
    "Cristiano Ronaldo dos Santos Aveiro (Portuguese pronunciation: [kɾiʃˈtjɐnu ʁɔˈnaldu]; born 5 February 1985) is a Portuguese professional footballer who plays as a forward for and captains both Saudi Pro League club Al Nassr and the Portugal national team. Widely regarded as one of the greatest players of all time, Ronaldo has won five Ballon d'Or awards,[note 3] a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship and the UEFA Nations League. Ronaldo holds the records for most appearances (183), goals (140) and assists (42) in the Champions League, goals in the European Championship (14), international goals (128) and international appearances (205). He is one of the few players to have made over 1,200 professional career appearances, the most by an outfield player, and has scored over 850 official senior career goals for club and country, making him the top goalscorer of all time.\n",
    "\"\"\"\n",
    "\n",
    "labels = [\"person\", \"award\", \"date\", \"competitions\", \"teams\"]\n",
    "\n",
    "entities = model.predict_entities(text,labels)\n",
    "\n",
    "for entity in entities:\n",
    "    # print(entity[\"text\"], \"=>\", entity[\"label\"])\n",
    "    print(entities)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zju_wck/miniconda3/envs/llm-merging/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/zju_wck/miniconda3/envs/llm-merging/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'start': 1, 'end': 20, 'text': 'Harilala Rasoanaivo', 'label': 'PERSON', 'score': 0.9991864562034607}, {'start': 106, 'end': 128, 'text': 'Rasoanaivo Enterprises', 'label': 'organization', 'score': 0.9995230436325073}, {'start': 133, 'end': 159, 'text': 'Lot II M 92 Antohomadinika', 'label': 'address', 'score': 0.9411612749099731}, {'start': 179, 'end': 196, 'text': '+261 32 22 345 67', 'label': 'phone number', 'score': 0.9696452021598816}, {'start': 230, 'end': 258, 'text': 'harilala.rasoanaivo@telma.mg', 'label': 'email', 'score': 0.9944571256637573}, {'start': 291, 'end': 302, 'text': '501-02-1234', 'label': 'social security number', 'score': 0.9772676825523376}], [{'start': 1, 'end': 20, 'text': 'Harilala Rasoanaivo', 'label': 'PERSON', 'score': 0.9991864562034607}, {'start': 106, 'end': 128, 'text': 'Rasoanaivo Enterprises', 'label': 'organization', 'score': 0.9995230436325073}, {'start': 133, 'end': 159, 'text': 'Lot II M 92 Antohomadinika', 'label': 'address', 'score': 0.9411612749099731}, {'start': 179, 'end': 196, 'text': '+261 32 22 345 67', 'label': 'phone number', 'score': 0.9696452021598816}, {'start': 230, 'end': 258, 'text': 'harilala.rasoanaivo@telma.mg', 'label': 'email', 'score': 0.9944571256637573}, {'start': 291, 'end': 302, 'text': '501-02-1234', 'label': 'social security number', 'score': 0.9772676825523376}]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(entities)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mentity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=>\u001b[39m\u001b[38;5;124m\"\u001b[39m, entity[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "dd = 'PERSON, organization, phone number, address, passport number, email, credit card number, social security number, health insurance id number, date of birth, mobile phone number, bank account number, medication, cpf, driver\\'s license number, tax identification number, medical condition, identity card number, national id number, ip address, email address, iban, credit card expiration date, username, health insurance number, registration number, student id number, insurance number, flight number, landline phone number, blood type, cvv, reservation number, digital signature, social media handle, license plate number, cnpj, postal code, passport_number, serial number, vehicle registration number, credit card brand, fax number, visa number, insurance company, identity document number, transaction number, national health insurance number, cvc, birth certificate number, train ticket number, passport expiration date,social_security_number'.replace(', ','|').split('|')\n",
    "dd\n",
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_multi_pii-v1\")\n",
    "\n",
    "text = [\"\"\"\n",
    "Harilala Rasoanaivo, un homme d'affaires local d'Antananarivo, a enregistré une nouvelle société nommée \"Rasoanaivo Enterprises\" au Lot II M 92 Antohomadinika. Son numéro est le +261 32 22 345 67, et son adresse électronique est harilala.rasoanaivo@telma.mg. Il a fourni son numéro de sécu 501-02-1234 pour l'enregistrement.\n",
    "\"\"\",\"\"\"\n",
    "Harilala Rasoanaivo, un homme d'affaires local d'Antananarivo, a enregistré une nouvelle société nommée \"Rasoanaivo Enterprises\" au Lot II M 92 Antohomadinika. Son numéro est le +261 32 22 345 67, et son adresse électronique est harilala.rasoanaivo@telma.mg. Il a fourni son numéro de sécu 501-02-1234 pour l'enregistrement.\n",
    "\"\"\",\n",
    "]\n",
    "labels = dd\n",
    "# labels = [\"work\", \"booking number\", \"personally identifiable information\", \"driver licence\", \"person\", \"book\", \"full address\", \"company\", \"actor\", \"character\", \"email\", \"passport number\", \"Social Security Number\", \"phone number\"]\n",
    "# entities = model.predict_entities(text, labels)\n",
    "entities = model.batch_predict_entities(text, labels)\n",
    "print(entities)\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harilala Rasoanaivo person 0.9999170303344727\n",
      "Rasoanaivo Enterprises organization 0.9993711113929749\n",
      "harilala.rasoanaivo@telma.mg email 0.999891996383667\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from gliner_spacy.pipeline import GlinerSpacy\n",
    "\n",
    "# Configuration for GLiNER integration\n",
    "custom_spacy_config = {\n",
    "    \"gliner_model\": \"urchade/gliner_multi_pii-v1\",\n",
    "    # \"chunk_size\": 250,\n",
    "    \"labels\": [\"person\", \"organization\", \"email\"],\n",
    "    \"style\": \"ent\",\n",
    "    \"threshold\": 0.8,\n",
    "    \"map_location\": \"cuda\" # only available in v.0.0.7\n",
    "}\n",
    "\n",
    "# Initialize a blank English spaCy pipeline and add GLiNER\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)\n",
    "\n",
    "\n",
    "# Example text for entity detection\n",
    "text = '''Harilala Rasoanaivo, un homme d'affaires local d'Antananarivo, a enregistré une nouvelle société nommée \"Rasoanaivo Enterprises\" au Lot II M 92 Antohomadinika. Son numéro est le +261 32 22 345 67, et son adresse électronique est harilala.rasoanaivo@telma.mg. Il a fourni son numéro de sécu 501-02-1234 pour l'enregistrement.\n",
    "'''\n",
    "\n",
    "# Process the text with the pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Output detected entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent._.score) # ent._.score only available in v. 0.0.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-merging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
