# Model arguments
model_name_or_path:   # modify
model_revision: main
# tokenizer_name_or_path: philschmid/gemma-tokenizer-chatml # Custom tokenizer with <|im_start|> and <|im_end|> tokens
tokenizer_name_or_path:   # modify
torch_dtype: bfloat16
attn_implementation: flash_attention_2
hub_token: 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

# Data training arguments
# dataset_mixer:
#   HuggingFaceH4/deita-10k-v0-sft: 1.0
# dataset_splits:
# - train_sft
# - test_sft
preprocessing_num_workers: 12
# dataset_dir: /zju_wck/gzy/Privacy/processed_data/SFTAB/sft_a.json
path_data:  # modify
name_data:  # modify
proc_data: sft-b-phishing-w # modify

# SFT trainer config
bf16: true
dataset_kwargs:
  add_special_tokens: false  # We already wrap <bos> and <eos> in the chat template
  append_concat_token: false # No need to add <eos> across samples
do_eval: true
eval_strategy: 'no' # modify epoch->no
gradient_accumulation_steps:  #modify
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
# hub_model_id: zephyr-7b-gemma-sft
# hub_strategy: every_save
learning_rate: 
log_level: info
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_seq_length:  # modify
max_steps: -1
num_train_epochs: 5
output_dir:  # modify
overwrite_output_dir: true  
per_device_eval_batch_size: # modify
per_device_train_batch_size: # modify
remove_unused_columns: true
report_to:
- wandb
save_strategy: "epoch"
seed: 42
warmup_ratio: 0.1
push_to_hub: False
alpha_pi: 0.3